import os
from Parsers import BaseParser,CNNParser
import json
from typing import Union, List
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver import ChromeOptions
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class NewsURLDownloader:
    '''ä¸‹è½½å™¨ç±»ï¼Œç”¨äºæ‰¹é‡ä¸‹è½½ç½‘é¡µå¹¶è§£æå†…å®¹ï¼Œå¤šè¿›ç¨‹å®ç°'''
    def __init__(self, urls:Union[str,List[str]], parserd_dir:str="parserd",ifDownload=True,user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36")->None:
        if isinstance(urls, str):
            self.urls = [urls]
        else:
            self.urls = urls
        self.parserd_dir = parserd_dir
        self.ifDownload=ifDownload
        self.user_agent=user_agent
        
    @staticmethod
    def _create_driver(user_agent)->webdriver.Chrome: 
        chromedriver_path = r"driver\chromedriver.exe" #æ­¤å¤„ä½¿ç”¨çš„æ˜¯å·²ç»ä¸‹è½½å¥½çš„chromedriver.exeè·¯å¾„ï¼Œéœ€è¦æ›´æ”¹ä¸ºè‡ªå·±çš„è·¯å¾„
        service = Service(chromedriver_path) 
        options = ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--ignore-certificate-errors")  #å¿½ç•¥ SSL é”™è¯¯
        options.add_argument("--disable-infobars")
        options.add_argument("--disable-extensions")
        options.add_argument(f'--user-agent={user_agent}')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument('log-level=3')
        prefs = { #ä¸åŠ è½½å›¾ç‰‡ï¼ŒèŠ‚çœå†…å­˜å’ŒåŠ è½½æ—¶é—´
            "profile.managed_default_content_settings.images": 2
        }
        options.add_experimental_option("prefs", prefs)
        return webdriver.Chrome(service=service, options=options)
    def _batch_process_worker(self, urls: List[str], worker_id: int, parser: BaseParser = None) -> dict:
        '''æ¯ä¸ªè¿›ç¨‹ä¸­è¿è¡Œï¼šåˆå§‹åŒ– driverï¼Œç„¶åæ‰¹é‡å¤„ç†ä¸€ä¸ª url å­é›†'''
        driver = self._create_driver(self.user_agent)
        result = {}
        total = len(urls)
        for i, url in enumerate(urls):
            try:
                logging.info(f"[çˆ¬è™« {worker_id}] [{i+1}/{total}] ğŸ” æ­£åœ¨æ£€æŸ¥é“¾æ¥ï¼š{url}")
                driver.get(url)
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                final_url = driver.current_url
                # ğŸ” å…ˆåˆ¤æ–­æœ€ç»ˆè·³è½¬ URL æ˜¯å¦å…è®¸è¢«æŠ“å–
                if parser:
                    if not parser.url_allowed(final_url):
                        msg = f"ç¦æ­¢æŠ“å–ï¼ˆç”± robots.txt é™åˆ¶ï¼‰ï¼š{final_url}"
                        logging.warning(f"[çˆ¬è™« {worker_id}] [{i+1}/{total}] â›” {msg}")
                        result[url] = {
                            'url': url,
                            'status': "illegal"
                        }
                        continue
                # âœ… åˆæ³•åå†æ¬¡è®¿é—®ä»¥è·å–å®Œæ•´ HTMLï¼ˆå¯ä»¥è·³è¿‡é‡å¤è·³è½¬ï¼Œæ€§èƒ½ä¼˜åŒ–ï¼‰
                logging.info(f"[çˆ¬è™« {worker_id}] [{i+1}/{total}] ğŸ”½ æŠ“å–å…è®¸çš„é¡µé¢ï¼š{final_url}")
                driver.get(final_url)
                WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                html = driver.page_source

                if parser:
                    parser.setURL(final_url)
                    parser.setHTML(html)
                    parsed = parser.parse()
                    result[url] = parsed
                    logging.info(f"[çˆ¬è™« {worker_id}] [{i+1}/{total}] ğŸ§  å·²å®Œæˆè§£æï¼š{final_url}")
                else:
                    result[url] = {"raw_html": html, "final_url": final_url}
                    logging.info(f"[çˆ¬è™« {worker_id}] [{i+1}/{total}] âœ… å·²å®ŒæˆæŠ“å–ï¼š{final_url}")
            except Exception as e:
                logging.warning(f"[çˆ¬è™« {worker_id}] [{i+1}/{total}] âŒ å¤„ç†å¤±è´¥ï¼š{url} - {e}")
                result[url] = {"error": str(e)}
        driver.quit()
        logging.info(f"[çˆ¬è™« {worker_id}] ğŸ›‘ å·²é€€å‡º")
        return result

    @staticmethod
    def _worker_run(urls: List[str], worker_id: int, parser: BaseParser = None) -> dict:
        '''ç‹¬ç«‹è¿›ç¨‹ä¸­è°ƒç”¨çš„å…¥å£ç‚¹ï¼Œç­‰æ•ˆäºä¸€ä¸ªçˆ¬è™«å·¥ä½œå™¨'''
        downloader = NewsURLDownloader(urls)
        return downloader._batch_process_worker(urls, worker_id, parser)
    def download(self, parser: BaseParser = None, output_file: str = "output.json", max_workers: int = 4) -> None:
        '''å°† URL å‡è¡¡åœ°åˆ†é…ç»™å¤šä¸ªå­è¿›ç¨‹ï¼Œä½¿æ¯ä¸ªå­è¿›ç¨‹ä»»åŠ¡å°½å¯èƒ½å¹³å‡ï¼Œå¯åŠ¨å¤šä¸ªçˆ¬è™«è¿›ç¨‹è¿›è¡Œè§£æ'''
        logging.info(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªçˆ¬è™«è¿›ç¨‹ï¼Œå‡†å¤‡åˆ†é… {len(self.urls)} ä¸ª URL")

        total_urls = len(self.urls)
        base = total_urls // max_workers
        extra = total_urls % max_workers
        url_chunks = []
        start = 0
        for i in range(max_workers):
            end = start + base + (1 if i < extra else 0)
            url_chunks.append(self.urls[start:end])
            start = end
        results = {}
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(NewsURLDownloader._worker_run, chunk, i, parser)
                for i, chunk in enumerate(url_chunks) if chunk  # é˜²æ­¢ç©ºä»»åŠ¡å—
            ]
            for future in as_completed(futures):
                res = future.result()
                results.update(res)
        failed_urls = [url for url, content in results.items() if "error" in content]
        if failed_urls:
            logging.warning(f"âš ï¸ æ€»å…±æœ‰ {len(failed_urls)} ä¸ª URL è§£æå¤±è´¥ã€‚")
        else:
            logging.info(f"âœ… å…¨éƒ¨URLéƒ½è§£ææˆåŠŸã€‚")
        output_path = os.path.join(self.parserd_dir, output_file)
        if self.ifDownload:
            os.makedirs(self.parserd_dir, exist_ok=True)
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            logging.info(f"âœ… å…¨éƒ¨æ•°æ®ä¿å­˜å®Œæˆï¼š{output_path}")
        else:
            logging.info(f"âœ… å…¨éƒ¨æ•°æ®å·²è§£æå®Œæˆï¼Œå·²ç»è¿”å›")
            return results

# æµ‹è¯•ç”¨ä¾‹
if __name__ == "__main__":
    urls = ["https://edition.cnn.com/2022/01/31/uk/boris-johnson-sue-gray-report-gbr-intl/index.html",
            "https://edition.cnn.com/2025/04/06/us/trump-crackdown-university-protests-student-activism-hnk/index.html",
            "https://edition.cnn.com/2022/01/31/europe/analysis-downing-street-lockdown-parties-big-deal/index.html",
            "https://edition.cnn.com/2025/04/07/science/dire-wolf-de-extinction-cloning-colossal/index.html",
            "https://edition.cnn.com/2025/04/07/business/china-trump-tariffs-opportunity-analysis-intl-hnk/index.html",
            "https://edition.cnn.com/2025/04/03/business/trumps-reciprocal-tariffs-countries-list-dg/index.html",
            "https://edition.cnn.com/2025/04/07/politics/trump-bessent-tariff-message/index.html",
            "https://edition.cnn.com/europe/live-news/boris-johnson-party-report-latest-intl-gbr/index.html",
            "https://edition.cnn.com/2022/02/26/europe/ukraine-russia-invasion-refugee-border-crossing-wait-kyiv-lviv-intl/index.html",
            "https://edition.cnn.com/2022/02/26/sport/james-harden-philadelphia-76ers-debut-nba-spt-intl/index.html",
            "https://edition.cnn.com/2022/02/25/sport/champions-league-final-russia-ukraine-sport-reaction-spt-intl/index.html",
            "https://edition.cnn.com/2022/02/25/europe/kyiv-ukraine-russian-invasion-mood-friday-intl/index.html",
            "https://edition.cnn.com/europe/live-news/ukraine-russia-news-02-23-22/index.html",
            "https://edition.cnn.com/2022/02/23/europe/ukraine-government-commercial-organizations-data-wiping-hack/index.html",
            "https://edition.cnn.com/science/gallery/black-rhino-photos-c2e-spc/index.html",
            "https://edition.cnn.com/profiles/anderson-cooper-profile"
            ]
    downloader = NewsURLDownloader(urls)
    downloader.download(CNNParser())